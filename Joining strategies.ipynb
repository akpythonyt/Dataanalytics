{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850045f9-bc60-40d6-840b-8ceaf0707767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data1 = [(1, \"A\"), (2, \"B\"), (3, \"C\")]\n",
    "data2 = [(1, \"X\"), (2, \"Y\"), (4, \"Z\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"val1\"])\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"val2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7444faec-5dbb-40d5-995c-4b930f679885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n| id|val1|val2|\n+---+----+----+\n|  1|   A|   X|\n|  2|   B|   Y|\n+---+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Broadcast the smaller dataframe\n",
    "df_broadcast = df1.join(broadcast(df2), \"id\", \"inner\")\n",
    "df_broadcast.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bc0f6d-9b37-49ed-b0b0-e4de334cceac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n| id|val1|val2|\n+---+----+----+\n|  1|   A|   X|\n|  2|   B|   Y|\n+---+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "# No explicit code - Spark uses this under the hood when appropriate\n",
    "# Just use a regular join and let Spark decide\n",
    "df_shuffle = df1.join(df2, \"id\", \"inner\")\n",
    "df_shuffle.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62e75bd-ebe7-4339-9c32-2d04cdaa4980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf7f486-2ed6-4980-b9ca-925268de1ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n| id|val1|val2|\n+---+----+----+\n|  1|   A|   X|\n|  2|   B|   Y|\n+---+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Disable broadcast to force sort-merge join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Run join normally\n",
    "df_sort_merge = df1.join(df2, \"id\", \"inner\")\n",
    "df_sort_merge.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2662b8bc-b564-42b9-bcdf-3846da0e0189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----+\n| id|val1| id|val2|\n+---+----+---+----+\n|  1|   A|  1|   X|\n|  1|   A|  2|   Y|\n|  1|   A|  4|   Z|\n|  2|   B|  1|   X|\n|  2|   B|  2|   Y|\n|  2|   B|  4|   Z|\n|  3|   C|  1|   X|\n|  3|   C|  2|   Y|\n|  3|   C|  4|   Z|\n+---+----+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "df_cross = df1.crossJoin(df2)\n",
    "df_cross.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738cf1b4-9f1b-42ea-95d8-cf7e875d744c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan (8)\n+- Project (7)\n   +- BroadcastHashJoin Inner BuildRight (6)\n      :- Filter (2)\n      :  +- Scan ExistingRDD (1)\n      +- Exchange (5)\n         +- Filter (4)\n            +- Scan ExistingRDD (3)\n\n\n(1) Scan ExistingRDD\nOutput [2]: [id#2L, val1#3]\nArguments: [id#2L, val1#3], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n\n(2) Filter\nInput [2]: [id#2L, val1#3]\nCondition : isnotnull(id#2L)\n\n(3) Scan ExistingRDD\nOutput [2]: [id#6L, val2#7]\nArguments: [id#6L, val2#7], MapPartitionsRDD[9] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n\n(4) Filter\nInput [2]: [id#6L, val2#7]\nCondition : isnotnull(id#6L)\n\n(5) Exchange\nInput [2]: [id#6L, val2#7]\nArguments: SinglePartition, EXECUTOR_BROADCAST, [plan_id=605]\n\n(6) BroadcastHashJoin\nLeft keys [1]: [id#2L]\nRight keys [1]: [id#6L]\nJoin type: Inner\nJoin condition: None\n\n(7) Project\nOutput [3]: [id#2L, val1#3, val2#7]\nInput [4]: [id#2L, val1#3, id#6L, val2#7]\n\n(8) AdaptiveSparkPlan\nOutput [3]: [id#2L, val1#3, val2#7]\nArguments: isFinalPlan=false\n\n\n"
     ]
    }
   ],
   "source": [
    "df_broadcast.explain(mode=\"formatted\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c2b1db-b383-4fe6-8773-d05c1eb77b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Joining strategies",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}